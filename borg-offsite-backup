#!/usr/bin/env python3

"""Borg Offsite Backup."""

import argparse
import contextlib
import fnmatch
import glob
import json
import os
import re
import shlex
import signal
import subprocess
import sys
import tempfile
import time
from collections.abc import Callable, Generator
from pathlib import Path
from subprocess import call
from subprocess import check_call as cc
from typing import Any, NamedTuple, TypedDict, TypeVar, cast

import dateutil.parser

# ruff: noqa: S607 # Starting a process with a partial executable path
# ruff: noqa: S603 # `subprocess` call: check for execution of untrusted input


TYPE_VOLUME = "volume"
TYPE_FILESYSTEM = "filesystem"

ZFS_SNAPSHOT_ROOT = ".borg-offsite-backup"
SNAPSHOT_PREFIX = "@borg-"
ZVOL_DEV_ROOT = "/dev/zvol"

DEFAULT_TELEMETRY_TIMEOUT = 600
SSH_OPTS = [
    "ServerAliveInterval 300",
    "ServerAliveCountMax 2",
    "ConnectTimeout 45",
]

echo: list[str] = []
terminated = False
dry_run = False


def sigterm(*_: Any) -> None:
    """Handle SIGTERM by setting terminated=True."""
    global terminated
    terminated = True


def status(string: str, *args: Any) -> None:
    if args:
        string = f"{string} {args}"
    if hasattr(sys.stderr, "flush"):
        sys.stderr.flush()


def sudo_ignore_dryrun(*cmd: str, **kwargs: Any) -> None:  # noqa: ANN401
    sudo_or_not = [] if os.getuid() == 0 else ["sudo"]
    cc((sudo_or_not) + list(cmd), **kwargs)


def sudo(*cmd: str, **kwargs: Any) -> None:  # noqa: ANN401
    """Run a command with sudo (if not already root)."""
    sudo_or_not = [] if os.getuid() == 0 else ["sudo"]
    if dry_run:
        status(f"Would run: {' '.join(sudo_or_not + list(cmd))}")
        return
    cc((sudo_or_not) + list(cmd), **kwargs)


def output_ignore_dryrun(*cmd: Any, **kwargs: Any) -> str:  # noqa: ANN401
    return subprocess.check_output(cmd, text=True, **kwargs)


def output(*cmd: Any, **kwargs: Any) -> str:  # noqa: ANN401
    """Run a command and return the output."""
    if dry_run:
        status(f"Would run: {' '.join(cmd)}")
        return ""
    return output_ignore_dryrun(*cmd, **kwargs)


def sudoutput(*cmd: Any) -> str:  # noqa: ANN401
    """Run a command with sudo and return the output."""
    sudo_or_not = [] if os.getuid() == 0 else ["sudo"]
    if dry_run:
        status(f"Would run: {' '.join(sudo_or_not + list(cmd))}")
        return ""
    lcmd: list[str] = (sudo_or_not) + list(cmd)
    return output(lcmd)


def dataset_exists(dataset_name: str) -> bool:
    """Check if a dataset exists.

    :param dataset_name: Name of the dataset.
    :return: True if the dataset exists, False otherwise.

    """
    return (
        call(
            ["zfs", "list", dataset_name],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        == 0
    )


def is_mountpoint(mp: str) -> bool:
    """Check if string is a mountpoint.

    :param mp: The mountpoint to check.
    :return: True if the string is a mountpoint, False otherwise.
    """
    return call(["mountpoint", "-q", "--", mp]) == 0


def list_props(
    dataset_name: str,
    props: list[str],
    *,
    recursive: bool,
) -> list[dict[str, str]]:
    """List properties of a ZFS dataset.

    :param dataset_name: The name of the dataset.
    :param props: The properties to list.
    :param recursive: Whether to list the properties recursively.
    :return: The properties of the dataset.

    """
    cmd = (
        ["zfs", "list", "-o", ",".join(props), "-H"]
        + (["-r"] if recursive else [])
        + [dataset_name]
    )
    return [
        dict(zip(props, s.split("\t"), strict=False))
        for s in output_ignore_dryrun(*cmd).splitlines()
        if s.strip()
    ]


def create_dataset(dataset: str) -> None:
    """Create a ZFS dataset."""
    sudo_ignore_dryrun(
        "zfs",
        "create",
        "-o",
        "com.sun:auto-snapshot=false",
        "-o",
        "org.qubes-os:part-of-qvm-pool=true",
        "-o",
        "secondarycache=metadata",
        "-o",
        "mountpoint=none",
        dataset,
    )


Y = TypeVar("Y")


def retrier(fun: Callable[..., Y], count: int, timeout: float) -> Y:
    """Retry a function until it succeeds or the count is reached.

    :param fun: The function to call.
    :param count: The number of retries.
    :param timeout: The time to wait between retries.
    :return: The result of the function.

    """
    if count < 1:
        msg = "count must be larger than zero"
        raise ValueError(msg)
    while True:
        try:
            return fun()
        except KeyboardInterrupt:  # noqa: PERF203 # no need for performance here
            raise
        except Exception:
            if count < 1:
                raise
            time.sleep(timeout)
            count = count - 1


def snapshot_exists(snapshot_name: str) -> bool:
    """Check if a ZFS snapshot exists.

    :param snapshot_name: The name of the snapshot.
    :return: True if the snapshot exists, False otherwise.

    """
    return (
        call(
            ["zfs", "list", "-t", "snapshot", snapshot_name],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        == 0
    )


def create_snapshot(snapshot_name: str) -> None:
    """Create a ZFS snapshot.

    :param snapshot_name: The name of the snapshot.

    """
    status(f"Snapshotting current {snapshot_name}")
    sudo_ignore_dryrun("zfs", "snapshot", snapshot_name)


def clone_snapshot(snapshot_name: str, dataset_name: str) -> None:
    """Clone a ZFS snapshot.

    :param snapshot_name: The name of the snapshot.
    :param dataset_name: The name of the dataset to clone to.

    """
    status(f"Cloning {snapshot_name} to {dataset_name}")
    sudo_ignore_dryrun(
        "zfs",
        "clone",
        "-p",
        snapshot_name,
        dataset_name,
    )


def mount_dataset(dataset_name: str, mountpoint: str) -> None:
    """Mount a ZFS dataset.

    :param dataset_name: The name of the dataset.
    :param mountpoint: The mountpoint to mount the dataset on.

    """
    status(f"Mounting {dataset_name} onto {mountpoint}")
    sudo_ignore_dryrun("zfs", "set", "mountpoint=" + mountpoint, dataset_name)


def unmount_dataset(dataset_name: str) -> None:
    """Unmount a ZFS dataset.

    :param dataset_name: The name of the dataset.

    """
    status(f"Unmounting dataset {dataset_name}")
    sudo_ignore_dryrun("zfs", "set", "mountpoint=none", dataset_name)


def destroy_dataset_recursively(dataset: str) -> None:
    """Destroy a ZFS dataset recursively.

    :param dataset: The name of the dataset.

    """
    status(f"Destroying {dataset} recursively")
    sudo_ignore_dryrun("zfs", "destroy", "-r", dataset)


@contextlib.contextmanager
def multi_context(*cms: Callable[[], Any]) -> Generator[Any, None, None]:
    """Create a context manager that manages multiple context managers.

    :param cms: The context managers to manage.
    :return: The results of the context managers.

    """
    with contextlib.ExitStack() as stack:
        yield [stack.enter_context(cls()) for cls in cms]


class DatasetToBackup:
    """A dataset to backup.

    :param name: The name of the dataset.
    :param recursive: Whether to backup the dataset recursively.
    :param glob: Whether to use globbing to match the dataset.

    """

    def __init__(  # noqa: D107
        self,
        name: str,
        *,
        recursive: bool,
        glob: bool,
    ) -> None:
        self.name = name
        if recursive and glob:
            msg = f"Invalid combination of recursive and glob for dataset {name}"
            raise ValueError(msg)
        self.recursive = recursive
        self.glob = glob


class Cfg:
    """Configuration for the backup.

    :param backup_path: Path to the backup.
    :param bridge_vm: Name of the bridge VM. Optional.
    :param backup_server: Name of the backup server.
    :param compression: Compression algorithm to use. Defaults to "auto,zstd".
    :param backup_user: User to use for the backup.
    :param keep_daily: Number of daily backups to keep. Defaults to 7.
    :param keep_weekly: Number of weekly backups to keep. Defaults to 4.
    :param keep_monthly: Number of monthly backups to keep. Defaults to 12.

    """

    backup_path: str = ""
    bridge_vm: str | None = None
    backup_server: str = ""
    compression = "auto,zstd"
    backup_user: str = ""
    keep_daily = 7
    keep_weekly = 4
    keep_monthly = 12

    def __init__(self) -> None:  # noqa: D107 # docstring in __init__
        self.filesystems_to_backup: list[str] = []
        self.datasets_to_backup: list[DatasetToBackup] = []
        self.exclude_patterns: list[str] = []

    def as_dict(self) -> dict[str, Any]:
        """Return the configuration as a dictionary.

        :return: The configuration as a dictionary.

        """
        d = {}
        for k in dir(self):
            v = getattr(self, k)
            if not callable(v):
                d[k] = v
        return d

    @classmethod
    def from_file(  # noqa: C901, PLR0912  # too complex
        cls: type["Cfg"],
        fn: str,
    ) -> "Cfg":
        """Create a configuration from a file.

        :param fn: The name of the file.
        :return: The configuration.

        """
        with open(fn, "r") as f:
            text = f.read()
        records = json.loads(text)
        keyvals = list(records.items())
        c = cls()
        for k, v in keyvals:
            if k in ("datasets_to_backup", "filesystems_to_backup", "exclude_patterns"):
                if hasattr(v, "items"):
                    v = list(v.keys())  # noqa: PLW2901
                if k == "datasets_to_backup":
                    dsets = []
                    for dset in v:
                        if isinstance(dset, str):
                            dsets.append(
                                DatasetToBackup(name=dset, recursive=False, glob=False),
                            )
                        elif isinstance(dset, dict):
                            dsets.append(
                                DatasetToBackup(
                                    name=dset["name"],
                                    recursive=dset.get("recursive", False),
                                    glob=dset.get("glob", False),
                                ),
                            )
                        else:
                            msg = f"Invalid dataset to backup: {dset}"
                            raise ValueError(msg)  # noqa: TRY004
                    c.datasets_to_backup = dsets
                else:
                    setattr(c, k, v)
            elif v:
                setattr(c, k, v)
        for p in "backup_path backup_server backup_user".split():
            if not getattr(c, p):
                msg = f"{p} must be defined in {fn}"
                raise ValueError(msg)
        if not c.datasets_to_backup and not c.filesystems_to_backup:
            msg = (
                "either datasets_to_backup or filesystems_to_backup "
                f"must be defined and nonempty in {fn}"
            )
            raise ValueError(
                msg,
            )
        return c


class SnapshotContext:
    """A context manager for creating and destroying ZFS snapshots.

    :param datasets_to_backup: The datasets to backup.
    :param execdate: The execution date.
    :param tmpdir: The temporary directory.

    """

    def __init__(  # noqa: D107
        self,
        datasets_to_backup: list[DatasetToBackup],
        execdate: str,
        tmpdir: str,
    ) -> None:
        self.datasets_to_backup: list[DatasetToBackup] = datasets_to_backup
        self.execdate = execdate
        self.tmpdir = tmpdir
        self.read_special = False

    def datasets_to_backup_sorted_by_mountpoint(self) -> list[tuple[str, str, str]]:
        """Get the datasets to backup sorted by mountpoint.

        Returns:
            list[tuple[str, str, str]]:
                A list of tuples with the dataset name, type and mountpoint path.

        """
        arrange = []
        already = set()
        for d in self.datasets_to_backup:
            if d.glob:
                dd = [
                    row
                    for row in list_props(
                        dataset_name=d.name.partition("/")[0],
                        props=["name", "mountpoint", "type"],
                        recursive=True,
                    )
                    if fnmatch.fnmatch(row["name"], d.name)
                ]
            else:
                dd = list_props(
                    dataset_name=d.name,
                    props=["name", "mountpoint", "type"],
                    recursive=d.recursive,
                )
            for dataset in dd:
                name, type_, mp = (
                    dataset["name"],
                    dataset["type"],
                    dataset["mountpoint"],
                )
                if name in already:
                    continue
                if type_ == "filesystem" and mp == "none":
                    continue
                arrange.append((name, type_, mp))
                already.add(name)
        return sorted(arrange, key=lambda x: x[1] + "/" + x[2])

    def roots(self) -> list[str]:
        """Get the root(s) of the datasets to backup."""
        return list(
            {
                str(Path(d.name.partition("/")[0]) / ZFS_SNAPSHOT_ROOT)
                for d in self.datasets_to_backup
            },
        )

    def snapshot_to_target(self, d: str) -> str:
        """Get the target of a snapshot.

        :param d: The snapshot.
        :return: The target of the snapshot.

        """
        return os.path.join(
            d.partition("/")[0],
            ZFS_SNAPSHOT_ROOT,
            d.partition("/")[2],
        ).rstrip("/")

    def __enter__(self) -> "SnapshotContext":
        """Create snapshots and mount datasets."""
        for root in self.roots():
            if not dataset_exists(root):
                create_dataset(root)

        for dataset_name, _, __ in self.datasets_to_backup_sorted_by_mountpoint():
            snapshot_name = dataset_name + SNAPSHOT_PREFIX + self.execdate
            target_name = self.snapshot_to_target(dataset_name)
            if not snapshot_exists(snapshot_name):
                create_snapshot(snapshot_name)

            if not dataset_exists(target_name):
                clone_snapshot(snapshot_name, target_name)

        for (
            dataset_name,
            dataset_type,
            mountpoint_path,
        ) in self.datasets_to_backup_sorted_by_mountpoint():
            snapshot_name = dataset_name + SNAPSHOT_PREFIX + self.execdate
            target_name = self.snapshot_to_target(dataset_name)

            if dataset_type == TYPE_VOLUME:
                self.read_special = True
                source_dev = os.path.join(ZVOL_DEV_ROOT, target_name)
                target_dev = self.tmpdir + os.path.join(ZVOL_DEV_ROOT, dataset_name)
                status("Creating device file %s from %s", target_dev, source_dev)
                sudo_ignore_dryrun("mkdir", "-p", "--", str(Path(target_dev).parent))

                def remove_and_copy(p: str, q: str) -> None:
                    sudo("rm", "-f", q)
                    sudo("cp", "-faL", p, q)

                retrier(
                    lambda source_dev=source_dev, target_dev=target_dev: remove_and_copy(
                        source_dev,
                        target_dev,
                    ),
                    20,
                    0.05,
                )
            else:
                mountpoint_path = self.tmpdir + mountpoint_path.rstrip("/")
                if not os.path.exists(mountpoint_path) or not is_mountpoint(
                    mountpoint_path,
                ):
                    mount_dataset(target_name, mountpoint_path)

        return self

    def __exit__(self, *unused: object) -> None:
        """Unmount and destroy the snapshots."""
        for dataset_name, dataset_type, mountpoint_path in reversed(
            self.datasets_to_backup_sorted_by_mountpoint(),
        ):
            mountpoint_path_tmpdir = self.tmpdir + mountpoint_path.rstrip("/")
            target_name = self.snapshot_to_target(dataset_name)
            if dataset_type == TYPE_VOLUME:
                target_dev = self.tmpdir + os.path.join(ZVOL_DEV_ROOT, dataset_name)
                if Path(target_dev).is_file():
                    status("Deleting device file %s", target_dev)
                    sudo_ignore_dryrun("rm", "-f", target_dev)
            elif is_mountpoint(mountpoint_path_tmpdir):
                unmount_dataset(target_name)

        for root in self.roots():
            if dataset_exists(root):
                destroy_dataset_recursively(root)

        for dataset_name, _, __ in reversed(
            self.datasets_to_backup_sorted_by_mountpoint(),
        ):
            snapshot_name = dataset_name + SNAPSHOT_PREFIX + self.execdate
            if snapshot_exists(snapshot_name):
                destroy_dataset_recursively(snapshot_name)


class BindMountContext:
    """Context manager for bind mounting file systems."""

    def __init__(  # noqa: D107
        self,
        filesystems_to_backup: list[str],
        tmpdir: str,
    ) -> None:
        self.filesystems_to_backup = filesystems_to_backup
        self.tmpdir = tmpdir

    def fsmounts(self) -> Generator[tuple[str, str], None, None]:
        """Get the file systems to mount."""
        for d in sorted(self.filesystems_to_backup):
            yield (d, str(Path(self.tmpdir) / d))

    def __enter__(self) -> None:  # noqa: D105
        needsmount = []
        for d, mp in self.fsmounts():
            if not Path(mp).exists() or not is_mountpoint(mp):
                needsmount.append((mp, d))
        for mp, d in sorted(needsmount):
            status("Creating %s", mp)
            if not Path(mp).exists():
                sudo_ignore_dryrun("mkdir", "-m", "700", "-p", "--", mp)
            if not is_mountpoint(mp):
                status("Mounting %s onto %s", d, mp)
                sudo_ignore_dryrun("mount", "--bind", d, mp)

    def __exit__(self, *unused: object) -> None:  # noqa: D105
        for _d, mp in reversed(list(self.fsmounts())):
            if is_mountpoint(mp):
                try:
                    status("Unmounting file system %s", mp)
                    sudo_ignore_dryrun("umount", mp)
                except subprocess.CalledProcessError as exc:
                    status("Unmounting file system %s failed: %s", mp, exc)
                    time.sleep(1)
                    continue


class TmpDirContext:
    """Context manager for creating and removing a temporary directory."""

    def __init__(self, tmpdir: str) -> None:  # noqa: D107
        self.tmpdir = tmpdir

    def __enter__(self) -> None:
        """Create temporary directory."""
        sudo_ignore_dryrun("mkdir", "-m", "700", "-p", "--", self.tmpdir)

    def __exit__(self, *_: object) -> None:
        """Remove the temporary directory if nothing is mounted into it.

        Raise an exception if file systems are still mounted on it.
        """
        if call(["findmnt", self.tmpdir]) == 0:
            msg = (
                "Will not remove anything under the temporary "
                "directory since file systems are still mounted on it."
            )
            raise RuntimeError(msg)

        try:
            status("Removing %s", self.tmpdir)
            qtmp = shlex.quote(self.tmpdir)
            sudo_ignore_dryrun(
                "bash",
                "-c",
                f"find {qtmp} -depth -type d -print0 | xargs -0 rmdir -v --",
            )
        except subprocess.CalledProcessError:
            status(output("find", self.tmpdir))
            status(output("findmnt", self.tmpdir))
            raise


def check_connectivity(cmd: list[str]) -> None:
    """Check connectivity to a host.

    Retries the command 10 times with a 3 second sleep in between until it succeeds.

    :param cmd: The command to check with for connectivity.

    """
    for _ in range(10):
        with open(os.devnull, "rb") as devnull:
            p = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                stdin=devnull,
                timeout=10,
                check=False,
            )
        if p.returncode != 0:
            time.sleep(3)
        else:
            break
    p.check_returncode()


def nc(host: str) -> list[str]:
    """Return a netcat command to check connectivity to a host."""
    return ["nc", "-w", "45", host, "22"]


class VmBridgeContext:
    """Context manager for connecting to a VM via a bridge."""

    def __init__(self, vm_name: str, host: str) -> None:  # noqa: D107
        self.host = host
        self.vm_name = vm_name
        self.was_running_before = False

    def __enter__(self) -> None:
        """Check things and set BORG_RSH envvar."""

        def wrap(cmd: list[str]) -> list[str]:
            return [
                "qvm-run",
                "--nogui",
                "-a",
                "--pass-io",
                self.vm_name,
                " ".join(shlex.quote(x) for x in cmd),
            ]

        if "Running" in output("qvm-ls", self.vm_name):
            self.was_running_before = True
        if "yes" not in output(*wrap(["echo", "yes"])):
            msg = f"VM {self.vm_name} not functional"
            raise RuntimeError(msg)

        check_connectivity(cmd=wrap(nc(self.host)))

        opts_list = []
        for opt in SSH_OPTS:
            opts_list.extend(["-o", opt])

        os.environ["BORG_RSH"] = "borg-offsite-backup-helper " + " ".join(
            shlex.quote(x) for x in [*opts_list, self.vm_name]
        )

    def __exit__(self, *_: object) -> None:
        """Unset BORG_RSH envvar, optionally shutdown qvm."""
        if "BORG_RSH" in os.environ:
            del os.environ["BORG_RSH"]
        if not self.was_running_before:
            cc(["qvm-shutdown", "--wait", self.vm_name])


class NoBridgeContext:
    """Context manager for connecting to a host directly."""

    def __init__(self, host: str) -> None:  # noqa: D107
        self.host = host

    def __enter__(self) -> None:
        """Check connection and set BORG_RSH envvar."""
        check_connectivity(cmd=nc(self.host))

        opts_list = []
        for opt in SSH_OPTS:
            opts_list.extend(["-o", opt])

        os.environ["BORG_RSH"] = "ssh " + " ".join(shlex.quote(x) for x in opts_list)

    def __exit__(self, *_: object) -> None:
        """Unset BORG_RSH environment variable."""
        if "BORG_RSH" in os.environ:
            del os.environ["BORG_RSH"]


@contextlib.contextmanager
def change_directory_context(path: str) -> Generator[str, None, None]:
    """Change the current working directory.

    :param path: The path to change to.
    """
    prev_cwd = Path.cwd()
    os.chdir(path)
    try:
        yield path
    finally:
        os.chdir(prev_cwd)


@contextlib.contextmanager
def dummy_context(*_: object) -> Generator[str, None, None]:  # noqa: D103
    yield "DummyContext"


def is_locked() -> bool:
    """Check if the repository is locked by borg."""
    return bool(
        glob.glob(
            os.path.join(
                os.path.expanduser("~"),
                ".cache",
                "borg",
                "*",
                "lock.exclusive",
            ),
        ),
    )


def unlock() -> None:
    """Unlock the borg repository via break-lock."""
    status("Unlocking repository")
    subprocess.check_call([*echo, "borg", "break-lock"])


def run(  # noqa: PLR0913
    config: Cfg,
    execdate: str,
    compression: str,
    snapshot_contexts: list[SnapshotContext],
    subcommand: str,
    *,
    read_special: bool,
    args: list[str],
) -> int:
    """Run a borg command.

    :param config: Configuration.
    :param execdate: Execution date.
    :param compression: Compression algorithm.
    :param snapshot_contexts: Snapshot contexts.
    :param subcommand: The borg subcommand.
    :param read_special: Whether to read special files.
    :param args: Additional arguments.
    :return: The return code of the command.

    """
    if is_locked():
        unlock()

    if subcommand == "create":
        with tempfile.TemporaryDirectory(
            prefix="borg-offsite-backup",
            suffix="XXXXXXXXXX",
        ) as excludedir:
            excludefn = str(Path(excludedir) / "exclude")
            with open(excludefn, "w") as excludefile:
                excludefile.write("\n".join(config.exclude_patterns))
                excludefile.flush()
            params = ["--progress"] if not os.environ.get("QUIET") else []
            if "comment" in output("borg", "create", "--help"):
                _snap_datasets: list[dict[str, str]] = []
                for _snap in snapshot_contexts:
                    for snap_ds in _snap.datasets_to_backup_sorted_by_mountpoint():
                        _snap_datasets.extend(
                            [{"dataset_name": snap_ds[0], "mountpoint": snap_ds[2]}],
                        )
                _comment = re.escape(
                    json.dumps(_snap_datasets, sort_keys=True, default=str),
                )
                params += ["--comment", f"'{_comment}'"]
            if "--sparse" in output("borg", "create", "--help"):
                params += ["--sparse"]
            if read_special:
                params += ["--read-special"]
            cmd = [
                "time",
                "nice",
                "ionice",
                "-c3",
                # "strace",
                # "-ff",
                # "-efile",
                # "-s2048",
                "borg",
                "create",
                "--exclude-caches",
                "--keep-exclude-tags",
                "--exclude-from",
                excludefn,
                "--debug",
                "--compression",
                compression,
                *params,
                *args,
                "::{{hostname}}_{{now}}",
                ".",
            ]
            if dry_run:
                status("Would run:", " ".join(cmd))
                ret = 0
            else:
                ret = subprocess.call(echo + cmd)
    else:
        cmd = ["borg", subcommand, *args]
        if dry_run:
            status("Would run:", " ".join(echo + cmd))
            ret = 0
        else:
            ret = subprocess.call(echo + cmd)
    if subcommand == "create" and ret == 1:
        # This is just a warning.
        ret = 0
    return ret


def run_prune(keep_daily: int, keep_weekly: int, keep_monthly: int) -> int:
    """Run borg prune.

    :param keep_daily: Number of daily backups to keep.
    :param keep_weekly: Number of weekly backups to keep.
    :param keep_monthly: Number of monthly backups to keep.
    :return: The return code of the command.

    """
    if is_locked():
        unlock()
    params = ["--progress"] if not os.environ.get("QUIET") else []
    cmd = [
        "time",
        "nice",
        "ionice",
        "-c3",
        "borg",
        "prune",
        "--glob-archives",
        f"{os.uname().nodename}-*",
        "--stats",
        "--keep-daily",
        str(keep_daily),
        "--keep-weekly",
        str(keep_weekly),
        "--keep-monthly",
        str(keep_monthly),
        *params,
    ]
    if dry_run:
        status("Would run:", " ".join(echo + cmd))
        return 0
    return subprocess.call(echo + cmd)


def run_collector(promfile: str, telemetry_timeout: int) -> None:  # noqa: C901
    """Run the telemetry collector.

    :param promfile: The Prometheus file.
    :param telemetry_timeout: The timeout for the telemetry.

    """

    def r2t(x: str) -> float:
        return cast(float, dateutil.parser.parse(x).timestamp())

    class Metric(NamedTuple):
        name: str
        labels: dict
        value: object

    def repr_metric(x: Metric) -> str:
        return "borg_" + str(x.name) + fmtlbs(x.labels) + " " + str(x.value)

    def fmtlbs(ls: dict[str, str]) -> str:
        if not ls:
            return ""
        lbs = "{"
        for n, (k, v) in enumerate(ls.items()):
            lbs += f'{k}="{v}"'
            if n < len(ls.items()) - 1:
                lbs += ","
        lbs += "}"
        return lbs

    metrics: list[Metric] = []
    p = metrics.append

    start = time.time()

    class Repository(TypedDict):
        id: str
        last_modified: str
        location: str

    class ArchiveStats(TypedDict):
        compressed_size: int
        deduplicated_size: int
        nfiles: int
        original_size: int

    class ArchiveDetail(TypedDict):
        comment: str
        duration: int
        end: str
        name: str
        start: str
        stats: ArchiveStats

    class ArchiveInfo(TypedDict):
        archives: list[ArchiveDetail]
        repository: Repository

    data = cast(
        ArchiveInfo,
        json.loads(
            subprocess.check_output(
                [sys.executable, sys.argv[0], "info", "--last=1000", "--json"],
                timeout=telemetry_timeout,
            ).decode("utf-8"),
        ),
    )
    p(
        Metric(
            name="repository_last_modified_timestamp_seconds",
            labels={},
            value=r2t(data["repository"]["last_modified"]),
        ),
    )

    for arcdata in data["archives"]:
        lbs = {"archive": arcdata["name"]}
        lbs["dataset"] = arcdata["comment"]

        p(
            Metric(
                name="archive_start_timestamp_seconds",
                labels=lbs,
                value=r2t(arcdata["start"]),
            ),
        )
        p(
            Metric(
                name="archive_end_timestamp_seconds",
                labels=lbs,
                value=r2t(arcdata["end"]),
            ),
        )

        archivestats = arcdata["stats"]
        for k, v in archivestats.items():
            p(
                Metric(
                    name="archive_%s"
                    % (
                        (k + "_bytes")
                        if ("size" in k)
                        else ("files" if k == "nfiles" else k)
                    ),
                    labels=lbs,
                    value=v,
                ),
            )

    end = time.time()
    p(Metric(name="collection_duration_seconds", labels={}, value=(end - start)))

    try:
        if promfile in ["/dev/stdout", "-"]:
            f = sys.stdout
        else:
            f = open(promfile + "." + str(os.getpid()), "w")  # noqa: SIM115
        for m in metrics:
            f.write(repr_metric(m) + "\n")
        if f != sys.stdout:
            f.flush()
            Path(promfile + "." + str(os.getpid())).rename(promfile)
    finally:
        if f != sys.stdout:
            f.close()


def parse_args(args: list[str]) -> tuple[argparse.Namespace, list[str]]:
    """Parse cmd args."""
    p = argparse.ArgumentParser()
    p.add_argument(
        "--telemetry-file",
        help="path to a file to save Node Exporter telemetry after creating a backup",
    )
    p.add_argument(
        "--telemetry-timeout",
        type=int,
        help="how long to wait until giving up on collecting telemetry",
        default=DEFAULT_TELEMETRY_TIMEOUT,
    )
    p.add_argument(
        "--config",
        help="path to configuration file",
        default="/etc/borg-offsite-backup.conf",
    )
    p.add_argument(
        "--dry-run",
        help="do not actually run any commands",
    )
    p.add_argument(
        "subcommand",
        help="borg execution mode",
    )
    return p.parse_known_args(args)


def main(args: list[str]) -> None:  # noqa: C901
    """Run main function."""
    global terminated

    if "--help" in args or "-h" in args or "-?" in args:
        sys.exit(subprocess.call(["borg", *args]))

    opts, borg_args = parse_args(args)

    signal.signal(signal.SIGTERM, sigterm)

    def fatal(exit_code: int, msg: str, *more: str) -> None:
        status(msg, *more)
        sys.exit(exit_code)

    if opts.dry_run:
        global dry_run
        dry_run = True
        status("Dry run enabled.")

    if opts.subcommand == "telemetry" and not opts.telemetry_file:
        fatal(
            os.EX_USAGE,
            "The telemetry subcommand is not supported without a "
            "--telemetry-file argument",
        )

    try:
        c = Cfg.from_file(opts.config)
    except ValueError as e:
        fatal(os.EX_USAGE, "Fatal error: " + str(e))

    execdate = output("date", "+%Y-%m-%d").strip()
    os.environ["BORG_KEY_FILE"] = str(Path("~/.borg-offsite-backup.key").expanduser())
    os.environ["BORG_PASSPHRASE"] = ""
    os.environ["BORG_REPO"] = "{backup_user}@{backup_server}:{backup_path}".format(
        **c.as_dict(),
    )
    if "TMPDIR" not in os.environ:
        os.environ["TMPDIR"] = tempfile.gettempdir()
    backup_root = "/run/borg-offsite-backup"

    backup_contexts: list[Callable[[], Any]] = []
    if opts.subcommand in ["create", "cleanup"]:
        backup_contexts.append(lambda: TmpDirContext(backup_root))
        if c.datasets_to_backup:
            backup_contexts.append(
                lambda: SnapshotContext(c.datasets_to_backup, execdate, backup_root),
            )
        if c.filesystems_to_backup:
            backup_contexts.append(
                lambda: BindMountContext(c.filesystems_to_backup, backup_root),
            )
        backup_contexts.append(
            lambda: change_directory_context(backup_root)
            if opts.subcommand == "create"
            else dummy_context(),
        )

    connect_context: Any = (
        VmBridgeContext(c.bridge_vm, c.backup_server)
        if c.bridge_vm is not None
        else NoBridgeContext(c.backup_server)
    )

    with connect_context:
        with multi_context(*backup_contexts) as context_results:
            snapshot_contexts = [
                c for c in context_results if isinstance(c, SnapshotContext)
            ]
            read_special = (
                snapshot_contexts[0].read_special if snapshot_contexts else False
            )
            if not terminated:
                retval = (
                    None
                    if opts.subcommand in ["cleanup", "telemetry"]
                    else run(
                        config=c,
                        execdate=execdate,
                        compression=c.compression,
                        snapshot_contexts=snapshot_contexts,
                        subcommand=opts.subcommand,
                        read_special=read_special,
                        args=borg_args,
                    )
                )
            if terminated and retval:
                retval = 0
                # Now wait so the service manager does not kill our cleanup right away.
                time.sleep(1)

        pruneretval = (
            run_prune(c.keep_daily, c.keep_weekly, c.keep_monthly)
            if (opts.subcommand == "create" and not terminated)
            else None
        )

        if (
            opts.telemetry_file
            and opts.subcommand in ["create", "telemetry", "delete", "rename", "prune"]
            and not terminated
        ):
            run_collector(opts.telemetry_file, opts.telemetry_timeout)

    for val, _ in [
        (retval, "Backup"),
        (pruneretval, "Prune"),
    ]:
        if val not in (0, None):
            sys.exit(val)


if __name__ == "__main__":
    main(sys.argv[1:])
